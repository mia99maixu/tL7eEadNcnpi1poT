{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2qfAtCQODFa",
        "outputId": "38028d9d-2687-4aae-dbf5-a067b5bcd774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                          job_title  \\\n",
            "72    73  Aspiring Human Resources Manager, seeking inte...   \n",
            "45    46              Aspiring Human Resources Professional   \n",
            "57    58              Aspiring Human Resources Professional   \n",
            "16    17              Aspiring Human Resources Professional   \n",
            "32    33              Aspiring Human Resources Professional   \n",
            "..   ...                                                ...   \n",
            "22    23    Advisory Board Member at Celal Bayar University   \n",
            "21    22             People Development Coordinator at Ryan   \n",
            "19    20  Native English Teacher at EPIK (English Progra...   \n",
            "17    18             People Development Coordinator at Ryan   \n",
            "103  104   Director Of Administration at Excellence Logging   \n",
            "\n",
            "                                location connection       fit  \n",
            "72                   Houston, Texas Area          7  1.000000  \n",
            "45   Raleigh-Durham, North Carolina Area         44  0.936737  \n",
            "57   Raleigh-Durham, North Carolina Area         44  0.936737  \n",
            "16   Raleigh-Durham, North Carolina Area         44  0.936737  \n",
            "32   Raleigh-Durham, North Carolina Area         44  0.936737  \n",
            "..                                   ...        ...       ...  \n",
            "22                        İzmir, Türkiye      500+   0.000000  \n",
            "21                         Denton, Texas      500+   0.000000  \n",
            "19                                Kanada      500+   0.000000  \n",
            "17                         Denton, Texas      500+   0.000000  \n",
            "103                          Katy, Texas      500+   0.000000  \n",
            "\n",
            "[104 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the dataset\n",
        "url = 'https://docs.google.com/spreadsheets/d/117X6i53dKiO7w6kuA1g1TpdTlv1173h_dPlJt5cNNMU/export?format=csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Define the keywords related to searching\n",
        "keywords = [\"aspiring human resources\", \"seeking human resources\"]\n",
        "\n",
        "# Combine the keywords into a single string for comparison\n",
        "keyword_str = \" \".join(keywords)\n",
        "\n",
        "# Step 1: Use TF-IDF to vectorize both job titles and keywords\n",
        "tfidf = TfidfVectorizer()\n",
        "job_titles_tfidf = tfidf.fit_transform(df['job_title'].fillna(''))\n",
        "keyword_tfidf = tfidf.transform([keyword_str])\n",
        "\n",
        "# Step 2: Calculate similarity between job titles and the keywords\n",
        "similarity_scores = cosine_similarity(job_titles_tfidf, keyword_tfidf).flatten()\n",
        "\n",
        "# Step 3: Normalize similarity scores\n",
        "scores = (similarity_scores - similarity_scores.min()) / (similarity_scores.max() - similarity_scores.min())\n",
        "\n",
        "# Step 4: Update the 'fit' column with the calculated fitness scores\n",
        "df['fit'] = scores\n",
        "\n",
        "# Step 5: Rank the candidates by fit score\n",
        "df_ranked = df.sort_values(by='fit', ascending=False)\n",
        "\n",
        "# Display the ranked dataset\n",
        "print(df_ranked[['id', 'job_title', 'location', 'connection', 'fit']])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNZD9Y9v_aY-",
        "outputId": "10d4f958-86b7-4e7c-ad13-29c24d02e046"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.1.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.1.0-py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Pre-trained SentenceTransformer model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Convert keywords into embeddings\n",
        "keyword_embeddings = model.encode(keywords)\n",
        "\n",
        "# Convert job titles into embeddings\n",
        "job_title_embeddings = model.encode(df['job_title'].fillna('').tolist())\n",
        "\n",
        "# Calculate cosine similarity between job titles and the keywords\n",
        "fit_scores = cosine_similarity(job_title_embeddings, np.mean(keyword_embeddings, axis=0).reshape(1, -1)).flatten()\n",
        "\n",
        "# Update the 'fit' column with the calculated fitness scores\n",
        "df['fit'] = fit_scores\n",
        "\n",
        "# View the dataset with fitness scores\n",
        "print(df[['id', 'job_title', 'fit']])\n",
        "\n",
        "# Rank the candidates by fit score\n",
        "df_ranked = df.sort_values(by='fit', ascending=False)\n",
        "\n",
        "# Display the ranked dataset\n",
        "print(df_ranked[['id', 'job_title', 'location', 'connection', 'fit']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDrOXHQa_mC4",
        "outputId": "0ec2c3aa-a1b8-44bf-b9ad-cdab730abb78"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                          job_title       fit\n",
            "0      1  2019 C.T. Bauer College of Business Graduate (...  0.502562\n",
            "1      2  Native English Teacher at EPIK (English Progra...  0.237043\n",
            "2      3              Aspiring Human Resources Professional  0.870847\n",
            "3      4             People Development Coordinator at Ryan  0.378838\n",
            "4      5    Advisory Board Member at Celal Bayar University  0.252375\n",
            "..   ...                                                ...       ...\n",
            "99   100  Aspiring Human Resources Manager | Graduating ...  0.722427\n",
            "100  101              Human Resources Generalist at Loparex  0.634931\n",
            "101  102   Business Intelligence and Analytics at Travelers  0.256748\n",
            "102  103                     Always set them up for Success  0.213875\n",
            "103  104   Director Of Administration at Excellence Logging  0.275158\n",
            "\n",
            "[104 rows x 3 columns]\n",
            "      id                                          job_title  \\\n",
            "29    30              Seeking Human Resources Opportunities   \n",
            "27    28              Seeking Human Resources Opportunities   \n",
            "98    99                   Seeking Human Resources Position   \n",
            "32    33              Aspiring Human Resources Professional   \n",
            "2      3              Aspiring Human Resources Professional   \n",
            "..   ...                                                ...   \n",
            "1      2  Native English Teacher at EPIK (English Progra...   \n",
            "19    20  Native English Teacher at EPIK (English Progra...   \n",
            "92    93  Admissions Representative at Community medical...   \n",
            "102  103                     Always set them up for Success   \n",
            "84    85  RRP Brand Portfolio Executive at JTI (Japan To...   \n",
            "\n",
            "                                location connection       fit  \n",
            "29                     Chicago, Illinois        390  0.935258  \n",
            "27                     Chicago, Illinois        390  0.935258  \n",
            "98                Las Vegas, Nevada Area         48  0.906348  \n",
            "32   Raleigh-Durham, North Carolina Area         44  0.870847  \n",
            "2    Raleigh-Durham, North Carolina Area         44  0.870847  \n",
            "..                                   ...        ...       ...  \n",
            "1                                 Kanada      500+   0.237043  \n",
            "19                                Kanada      500+   0.237043  \n",
            "92                Long Beach, California          9  0.227072  \n",
            "102             Greater Los Angeles Area      500+   0.213875  \n",
            "84             Greater Philadelphia Area      500+   0.167977  \n",
            "\n",
            "[104 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the 7th candidate and set 'starred' to 1\n",
        "df.loc[df_ranked.index[6], 'starred'] = 1\n",
        "df['starred'] = df['starred'].fillna(0)\n",
        "\n",
        "# Use the SentenceTransformer model (assuming it's still named 'model')\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Calculate the mean embedding of starred candidates\n",
        "starred_embeddings = model.encode(df[df['starred'] == 1]['job_title'].fillna('').tolist())\n",
        "\n",
        "if len(starred_embeddings) > 0:\n",
        "  starred_embeddings = np.mean(starred_embeddings, axis=0)\n",
        "else:\n",
        "  starred_embeddings = np.zeros(model.vector_size)\n",
        "\n",
        "# Calculate similarity with starred candidates\n",
        "fit_scores = cosine_similarity(job_title_embeddings, starred_embeddings.reshape(1, -1)).flatten()\n",
        "\n",
        "# Update the 'fit' column with the calculated fitness scores\n",
        "df['fit'] = fit_scores\n",
        "\n",
        "# Rank the candidates by fit score\n",
        "df_ranked = df.sort_values(by='fit', ascending=False)\n",
        "\n",
        "# Display the ranked dataset\n",
        "print(df_ranked[['id', 'job_title', 'location', 'connection', 'fit']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBkWABb50LAs",
        "outputId": "58776168-ca54-4022-9956-ddb5bd7e0658"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                          job_title  \\\n",
            "20    21              Aspiring Human Resources Professional   \n",
            "96    97              Aspiring Human Resources Professional   \n",
            "2      3              Aspiring Human Resources Professional   \n",
            "57    58              Aspiring Human Resources Professional   \n",
            "16    17              Aspiring Human Resources Professional   \n",
            "..   ...                                                ...   \n",
            "1      2  Native English Teacher at EPIK (English Progra...   \n",
            "15    16  Native English Teacher at EPIK (English Progra...   \n",
            "19    20  Native English Teacher at EPIK (English Progra...   \n",
            "84    85  RRP Brand Portfolio Executive at JTI (Japan To...   \n",
            "102  103                     Always set them up for Success   \n",
            "\n",
            "                                location connection       fit  \n",
            "20   Raleigh-Durham, North Carolina Area         44  0.948653  \n",
            "96                  Kokomo, Indiana Area         71  0.948653  \n",
            "2    Raleigh-Durham, North Carolina Area         44  0.948653  \n",
            "57   Raleigh-Durham, North Carolina Area         44  0.948653  \n",
            "16   Raleigh-Durham, North Carolina Area         44  0.948653  \n",
            "..                                   ...        ...       ...  \n",
            "1                                 Kanada      500+   0.257461  \n",
            "15                                Kanada      500+   0.257461  \n",
            "19                                Kanada      500+   0.257461  \n",
            "84             Greater Philadelphia Area      500+   0.213630  \n",
            "102             Greater Los Angeles Area      500+   0.182961  \n",
            "\n",
            "[104 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "\n",
        "\n",
        "model = Word2Vec(sentences=df['job_title'].fillna('').tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "# Calculate keyword embeddings, handling cases with no matching words\n",
        "keyword_embeddings = []\n",
        "for word in keywords:\n",
        "    if word in model.wv:\n",
        "        keyword_embeddings.append(model.wv[word])\n",
        "# Calculate the mean of keyword embeddings only if keyword_embeddings is not empty\n",
        "if keyword_embeddings:\n",
        "    keyword_embeddings = np.mean(keyword_embeddings, axis=0)\n",
        "else:\n",
        "    # Use a zero vector if no keywords are found in the vocabulary\n",
        "    keyword_embeddings = np.zeros(model.vector_size)\n",
        "\n",
        "# Calculate job title embeddings, handling cases with no matching words\n",
        "job_title_embeddings = []\n",
        "for job_title in df['job_title'].fillna('').tolist():\n",
        "  word_embeddings = [model.wv[word] for word in job_title.split() if word in model.wv]\n",
        "  if word_embeddings:\n",
        "    job_title_embeddings.append(np.mean(word_embeddings, axis=0))\n",
        "  else:\n",
        "    # Use a zero vector if no words are found in the vocabulary\n",
        "    job_title_embeddings.append(np.zeros(model.vector_size))\n",
        "\n",
        "# Convert job_title_embeddings to a NumPy array with consistent shape\n",
        "job_title_embeddings = np.array(job_title_embeddings)\n",
        "\n",
        "fit_scores = cosine_similarity(job_title_embeddings, keyword_embeddings.reshape(1, -1)).flatten()\n",
        "df['fit'] = fit_scores\n",
        "df_ranked = df.sort_values(by='fit', ascending=False)\n",
        "print(df_ranked[['id', 'job_title', 'location', 'connection', 'fit']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJwsTW1fp4QA",
        "outputId": "6d891694-5f57-4f9c-9834-721b98b0d698"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                          job_title  \\\n",
            "0      1  2019 C.T. Bauer College of Business Graduate (...   \n",
            "1      2  Native English Teacher at EPIK (English Progra...   \n",
            "76    77  Human Resources|\\nConflict Management|\\nPolici...   \n",
            "75    76  Aspiring Human Resources Professional | Passio...   \n",
            "74    75  Nortia Staffing is seeking Human Resources, Pa...   \n",
            "..   ...                                                ...   \n",
            "31    32  Native English Teacher at EPIK (English Progra...   \n",
            "30    31  2019 C.T. Bauer College of Business Graduate (...   \n",
            "29    30              Seeking Human Resources Opportunities   \n",
            "28    29  Aspiring Human Resources Management student se...   \n",
            "103  104   Director Of Administration at Excellence Logging   \n",
            "\n",
            "                   location connection  fit  \n",
            "0            Houston, Texas         85  0.0  \n",
            "1                    Kanada      500+   0.0  \n",
            "76   Dallas/Fort Worth Area        409  0.0  \n",
            "75       New York, New York        212  0.0  \n",
            "74     San Jose, California      500+   0.0  \n",
            "..                      ...        ...  ...  \n",
            "31                   Kanada      500+   0.0  \n",
            "30           Houston, Texas         85  0.0  \n",
            "29        Chicago, Illinois        390  0.0  \n",
            "28      Houston, Texas Area      500+   0.0  \n",
            "103             Katy, Texas      500+   0.0  \n",
            "\n",
            "[104 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fasttext to calculate the similarity\n",
        "model = FastText(sentences=df['job_title'].fillna('').tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "keyword_embeddings = []\n",
        "for word in keywords:\n",
        "    if word in model.wv:\n",
        "        keyword_embeddings.append(model.wv[word])\n",
        "if keyword_embeddings:\n",
        "    keyword_embeddings = np.mean(keyword_embeddings, axis=0)\n",
        "\n",
        "job_title_embeddings = []\n",
        "for job_title in df['job_title'].fillna('').tolist():\n",
        "  word_embeddings = [model.wv[word] for word in job_title.split() if word in model.wv]\n",
        "  if word_embeddings:\n",
        "    job_title_embeddings.append(np.mean(word_embeddings, axis=0))\n",
        "\n",
        "job_title_embeddings = np.array(job_title_embeddings)\n",
        "\n",
        "fit_scores = cosine_similarity(job_title_embeddings, keyword_embeddings.reshape(1, -1)).flatten()\n",
        "df['fit'] = fit_scores\n",
        "df_ranked = df.sort_values(by='fit', ascending=False)\n",
        "print(df_ranked[['id', 'job_title', 'location', 'connection', 'fit']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThLMQ0_uwuT2",
        "outputId": "55e84563-ae79-45dd-a52e-1578bcb0842c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    id                                          job_title  \\\n",
            "48  49                Aspiring Human Resources Specialist   \n",
            "5    6                Aspiring Human Resources Specialist   \n",
            "35  36                Aspiring Human Resources Specialist   \n",
            "23  24                Aspiring Human Resources Specialist   \n",
            "59  60                Aspiring Human Resources Specialist   \n",
            "..  ..                                                ...   \n",
            "54  55  SVP, CHRO, Marketing & Communications, CSR Off...   \n",
            "63  64  SVP, CHRO, Marketing & Communications, CSR Off...   \n",
            "85  86  Information Systems Specialist and Programmer ...   \n",
            "76  77  Human Resources|\\nConflict Management|\\nPolici...   \n",
            "95  96  Student at Indiana University Kokomo - Busines...   \n",
            "\n",
            "                      location connection       fit  \n",
            "48  Greater New York City Area          1  0.496755  \n",
            "5   Greater New York City Area          1  0.496755  \n",
            "35  Greater New York City Area          1  0.496755  \n",
            "23  Greater New York City Area          1  0.496755  \n",
            "59  Greater New York City Area          1  0.496755  \n",
            "..                         ...        ...       ...  \n",
            "54         Houston, Texas Area      500+  -0.228368  \n",
            "63         Houston, Texas Area      500+  -0.228368  \n",
            "85      Gaithersburg, Maryland          4 -0.230654  \n",
            "76      Dallas/Fort Worth Area        409 -0.240372  \n",
            "95          Lafayette, Indiana         19 -0.243459  \n",
            "\n",
            "[104 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Train the FastText model on job titles\n",
        "model = FastText(sentences=df['job_title'].fillna('').tolist(), vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 1: Calculate the embedding for job titles\n",
        "job_title_embeddings = []\n",
        "for job_title in df['job_title'].fillna('').tolist():\n",
        "    word_embeddings = [model.wv[word] for word in job_title.split() if word in model.wv]\n",
        "    if word_embeddings:\n",
        "        job_title_embeddings.append(np.mean(word_embeddings, axis=0))\n",
        "    else:\n",
        "        job_title_embeddings.append(np.zeros(model.vector_size))\n",
        "\n",
        "job_title_embeddings = np.array(job_title_embeddings)\n",
        "\n",
        "# Normalize the job title embeddings for similarity comparison\n",
        "job_title_embeddings = normalize(job_title_embeddings)\n",
        "\n",
        "# Step 2: Star the 7th candidate (or any chosen candidate)\n",
        "# We assume the 7th candidate is starred\n",
        "starred_candidate_index = 6\n",
        "starred_candidate_embedding = job_title_embeddings[starred_candidate_index].reshape(1, -1)\n",
        "\n",
        "# Step 3: Re-rank the candidates based on similarity to the starred candidate\n",
        "# Calculate cosine similarity between all job titles and the starred candidate's job title\n",
        "fit_scores = cosine_similarity(job_title_embeddings, starred_candidate_embedding).flatten()\n",
        "\n",
        "# Step 4: Add the fit scores to the DataFrame and sort by fit score\n",
        "df['fit'] = fit_scores\n",
        "df_ranked = df.sort_values(by='fit', ascending=False)\n",
        "\n",
        "# Output the re-ranked results\n",
        "print(df_ranked[['id', 'job_title', 'location', 'connection', 'fit']])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f1ofaxDzpd8",
        "outputId": "38f63a27-f3d8-482d-9077-3b58e939c4b2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id                                          job_title  \\\n",
            "36    37  Student at Humber College and Aspiring Human R...   \n",
            "6      7  Student at Humber College and Aspiring Human R...   \n",
            "38    39  Student at Humber College and Aspiring Human R...   \n",
            "51    52  Student at Humber College and Aspiring Human R...   \n",
            "49    50  Student at Humber College and Aspiring Human R...   \n",
            "..   ...                                                ...   \n",
            "41    42  SVP, CHRO, Marketing & Communications, CSR Off...   \n",
            "63    64  SVP, CHRO, Marketing & Communications, CSR Off...   \n",
            "74    75  Nortia Staffing is seeking Human Resources, Pa...   \n",
            "76    77  Human Resources|\\nConflict Management|\\nPolici...   \n",
            "102  103                     Always set them up for Success   \n",
            "\n",
            "                     location connection       fit  \n",
            "36                     Kanada         61  1.000000  \n",
            "6                      Kanada         61  1.000000  \n",
            "38                     Kanada         61  1.000000  \n",
            "51                     Kanada         61  1.000000  \n",
            "49                     Kanada         61  1.000000  \n",
            "..                        ...        ...       ...  \n",
            "41        Houston, Texas Area      500+  -0.068181  \n",
            "63        Houston, Texas Area      500+  -0.068181  \n",
            "74       San Jose, California      500+  -0.068594  \n",
            "76     Dallas/Fort Worth Area        409 -0.075272  \n",
            "102  Greater Los Angeles Area      500+  -0.134105  \n",
            "\n",
            "[104 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are interested in a robust algorithm, tell us how your solution works and show us how your ranking gets better with each starring action.\n",
        "\n",
        "* After starred one of the candidate the re-rank model come to calculate the similerity more relevan to the data point, so suggest to mannual select the top candidate as the model criteria\n",
        "\n",
        "How can we filter out candidates which in the first place should not be in this list?\n",
        "\n",
        "* Set the condition such like location or connection over 500.\n",
        "* Remove the candidates not meet the reqirement\n",
        "* Train a model from historical data\n",
        "\n",
        "Can we determine a cut-off point that would work for other roles without losing high potential candidates?\n",
        "\n",
        "* Instead of manually setting a cut-off point, use historical data to learn what an effective cut-off point looks like for different roles. This approach can help minimize the chances of losing high-potential candidates.\n",
        "\n",
        "Do you have any ideas that we should explore so that we can even automate this procedure to prevent human bias?\n",
        "\n",
        "* Train the model multi time with slightly change of the criteria, or apply different technique for training"
      ],
      "metadata": {
        "id": "7bp00niNx8B2"
      }
    }
  ]
}